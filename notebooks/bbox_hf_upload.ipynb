{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlhf.hugging_face import HuggingFace\n",
    "from vlhf.visual_layer import VisualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Authentication\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "VL_USER_ID = os.getenv(\"VL_USER_ID\")\n",
    "VL_ENV = os.getenv(\"VL_ENV\")\n",
    "VL_PG_URI = os.getenv(\"VL_PG_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 14:07:18.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.hugging_face\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mHugging Face session created\u001b[0m\n",
      "\u001b[32m2024-08-08 14:07:18.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVisual Layer session created\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "hf = HuggingFace(HF_TOKEN)\n",
    "vl = VisualLayer(VL_USER_ID, VL_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"f0eb6f74-5495-11ef-aab9-42e5998eb94f\" # german traffic sign dataset\n",
    "# hf_repo_id = \"dnth/german-traffic-sign-vl-enriched\"\n",
    "\n",
    "# dataset_id = \"c2c3b90a-4f39-11ef-8d8b-5e82a4538d0f\" # pokemon dataset\n",
    "# hf_repo_id = \"dnth/pokemon-vl-enriched\"\n",
    "\n",
    "# dataset_id = \"df7908dc-3df2-11ef-a272-b60519d00142\" # pets enriched dataset\n",
    "# hf_repo_id = \"dnth/pets-vl-enriched\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 14:07:18.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36mget_dataset\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mFetching dataset: df7908dc-3df2-11ef-a272-b60519d00142\u001b[0m\n",
      "\u001b[32m2024-08-08 14:07:22.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36m_get_images\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mRetrieved and processed 7384 images\u001b[0m\n",
      "\u001b[32m2024-08-08 14:07:26.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36m_get_image_labels\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mRetrieved 7349 image labels\u001b[0m\n",
      "\u001b[32m2024-08-08 14:07:31.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36m_get_object_labels\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mRetrieved 6824 object labels\u001b[0m\n",
      "\u001b[32m2024-08-08 14:07:34.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36m_get_image_issues\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1mRetrieved 1097 image issues\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7_384, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>image_uri</th><th>image_label</th><th>image_issues</th><th>object_labels</th></tr><tr><td>str</td><td>str</td><td>list[struct[4]]</td><td>list[struct[3]]</td></tr></thead><tbody><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[19, 16, … 359],&quot;377ded62-5b5b-4202-b338-5aa9e55f8dc0&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;strap&quot;,[133, 202, … 167],&quot;68f5737a-a50e-4ec8-8693-412bb0027c9f&quot;}, {&quot;strap&quot;,[201, 68, … 302],&quot;9bb31feb-8026-42cf-aa92-89b837db6bf3&quot;}, {&quot;dog&quot;,[69, 110, … 198],&quot;54f45449-9e6a-4f26-84e6-4c05b43ab481&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>[{&quot;dog&quot;,[48, 43, … 337],&quot;4c960e31-3f39-46e3-bf10-69e6fcea6e1a&quot;}, {&quot;person&quot;,[125, 235, … 265],&quot;cd6be9de-b992-4f9d-b163-ac35b0af801b&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[0, 4, … 267],&quot;371aa604-30a5-492f-95a5-eaf5aad34c37&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>[{&quot;tag&quot;,[1, 216, … 61],&quot;371dd660-79e2-4b8e-932f-5b9fd0750945&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[159, 74, … 266],&quot;33aa75ae-f598-4eb6-b986-404fb5a08390&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>[{&quot;cat&quot;,[1, 19, … 481],&quot;53e7a4dc-ee24-4fb5-9bef-2d3f8d1b8ef1&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;bear&quot;,[82, 96, … 398],&quot;03e3bbae-69e5-462a-85b5-816d3d310e25&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[94, 27, … 305],&quot;a57c3a15-bc52-4037-944e-5861b81a503b&quot;}, {&quot;pillow&quot;,[389, 220, … 74],&quot;86a2207f-1347-49e9-9a77-e424d51df229&quot;}]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[19, 35, … 376],&quot;6742b957-6346-4458-8e44-98a6f75aebe5&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[133, 47, … 152],&quot;56f3aed0-788c-4317-9202-a77405dcc8e2&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>[{&quot;cupboard&quot;,[20, 253, … 115],&quot;c5dae64e-7ca1-415a-9ecf-ce5533265139&quot;}, {&quot;cupboard&quot;,[80, 317, … 52],&quot;22171958-507c-41e6-ad63-252642b77917&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;cupboard&quot;,[0, 1, … 251],&quot;5f0194d8-f3fc-4f4e-bd59-005718061b87&quot;}, {&quot;dog&quot;,[33, 50, … 450],&quot;08e1fe13-eb26-4f18-a397-6e9c8ee106da&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[179, 65, … 278],&quot;583f5976-fbda-4ff8-895b-8683bbc344e5&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;basket&quot;,[227, 1, … 135],&quot;8a4a83bf-3685-41e7-a3db-fff53be5d1c8&quot;}, {&quot;dog&quot;,[50, 1, … 448],&quot;b1da16fb-8cc2-4d18-b966-fb440ffc02d6&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[77, 126, … 281],&quot;1ce28c22-36f1-4455-8d28-32aff1587b1c&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[0, 109, … 266],&quot;101aa240-e572-4cb5-bcd3-8d5311e23e64&quot;}, {&quot;cushion&quot;,[351, 266, … 109],&quot;9b4c275d-2843-4b97-9846-bf673aaf5aa2&quot;}, … {&quot;cushion&quot;,[0, 250, … 125],&quot;99175d44-a55f-49db-a1a6-e052eff5a4a3&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;cat&quot;,[31, 60, … 417],&quot;767d8f4d-8a2e-4f9d-8f01-0aeb74eb8508&quot;}, {&quot;ball&quot;,[79, 290, … 72],&quot;584c65e1-dae8-49ba-a3e4-32958afa8fec&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;cat&quot;</td><td>null</td><td>[{&quot;tank top&quot;,[2, 1, … 234],&quot;653d72f2-7690-400e-921a-ebc4d4054a88&quot;}, {&quot;cat&quot;,[0, 78, … 249],&quot;101f85ea-cc47-45f4-969c-5bd34f86e6d3&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[3, 11, … 270],&quot;91167ffe-cbd5-4445-a0a9-787d60de35c6&quot;}]</td></tr><tr><td>&quot;https://d2iycf…</td><td>&quot;dog&quot;</td><td>null</td><td>[{&quot;dog&quot;,[15, 32, … 241],&quot;1773a857-3e98-4793-a5bc-df6f5a1f1f1c&quot;}]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7_384, 4)\n",
       "┌───────────────────────────────────┬─────────────┬─────────────────┬────────────────────────┐\n",
       "│ image_uri                         ┆ image_label ┆ image_issues    ┆ object_labels          │\n",
       "│ ---                               ┆ ---         ┆ ---             ┆ ---                    │\n",
       "│ str                               ┆ str         ┆ list[struct[4]] ┆ list[struct[3]]        │\n",
       "╞═══════════════════════════════════╪═════════════╪═════════════════╪════════════════════════╡\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ cat         ┆ null            ┆ null                   │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ dog         ┆ null            ┆ [{\"dog\",[19, 16, …     │\n",
       "│                                   ┆             ┆                 ┆ 359],\"377ded6…         │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ dog         ┆ null            ┆ [{\"strap\",[133, 202, … │\n",
       "│                                   ┆             ┆                 ┆ 167],\"68f…             │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ cat         ┆ null            ┆ [{\"dog\",[48, 43, …     │\n",
       "│                                   ┆             ┆                 ┆ 337],\"4c960e3…         │\n",
       "│ …                                 ┆ …           ┆ …               ┆ …                      │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ dog         ┆ null            ┆ [{\"cat\",[31, 60, …     │\n",
       "│                                   ┆             ┆                 ┆ 417],\"767d8f4…         │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ cat         ┆ null            ┆ [{\"tank top\",[2, 1, …  │\n",
       "│                                   ┆             ┆                 ┆ 234],\"653d…            │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ dog         ┆ null            ┆ [{\"dog\",[3, 11, …      │\n",
       "│                                   ┆             ┆                 ┆ 270],\"91167ffe…        │\n",
       "│ https://d2iycffepdu1yp.cloudfron… ┆ dog         ┆ null            ┆ [{\"dog\",[15, 32, …     │\n",
       "│                                   ┆             ┆                 ┆ 241],\"1773a85…         │\n",
       "└───────────────────────────────────┴─────────────┴─────────────────┴────────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = vl._get_object_labels(dataset_id=dataset_id, pg_uri=VL_PG_URI)\n",
    "df = vl.get_dataset(dataset_id=dataset_id, pg_uri=VL_PG_URI)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7c2352f47245a2afd9c2decc8a9ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/7384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 14:07:38.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlhf.visual_layer\u001b[0m:\u001b[36mto_hf\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPushing dataset to HF repository: dnth/pets-vl-enriched\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf130578ab345219043c3dc17bf1991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249e86725c7549a2a28ca29e93bc4bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2: In chunk 0: Invalid: First or last list offset out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vl-hf-workflow/src/vlhf/visual_layer.py:71\u001b[0m, in \u001b[0;36mVisualLayer.to_hf\u001b[0;34m(self, hf_session, hf_repo_id, vl_dataset_df)\u001b[0m\n\u001b[1;32m     68\u001b[0m hf_session\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m     70\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPushing dataset to HF repository: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/datasets/arrow_dataset.py:5629\u001b[0m, in \u001b[0;36mDataset.push_to_hub\u001b[0;34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, branch, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_dir:\n\u001b[1;32m   5627\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m config_name \u001b[38;5;28;01mif\u001b[39;00m config_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m-> 5629\u001b[0m additions, uploaded_size, dataset_nbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5632\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5638\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5641\u001b[0m \u001b[38;5;66;03m# Check if the repo already has a README.md and/or a dataset_infos.json to update them with the new split info (size and pattern)\u001b[39;00m\n\u001b[1;32m   5642\u001b[0m \u001b[38;5;66;03m# and delete old split shards (if they exist)\u001b[39;00m\n\u001b[1;32m   5643\u001b[0m repo_with_dataset_card, repo_with_dataset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/datasets/arrow_dataset.py:5457\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, data_dir, split, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5455\u001b[0m shard_path_in_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-of-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shards\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5456\u001b[0m buffer \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m-> 5457\u001b[0m \u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5458\u001b[0m uploaded_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   5459\u001b[0m shard_addition \u001b[38;5;241m=\u001b[39m CommitOperationAdd(path_in_repo\u001b[38;5;241m=\u001b[39mshard_path_in_repo, path_or_fileobj\u001b[38;5;241m=\u001b[39mbuffer)\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/datasets/arrow_dataset.py:5180\u001b[0m, in \u001b[0;36mDataset.to_parquet\u001b[0;34m(self, path_or_buf, batch_size, storage_options, **parquet_writer_kwargs)\u001b[0m\n\u001b[1;32m   5175\u001b[0m \u001b[38;5;66;03m# Dynamic import to avoid circular dependency\u001b[39;00m\n\u001b[1;32m   5176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParquetDatasetWriter\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParquetDatasetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparquet_writer_kwargs\u001b[49m\n\u001b[0;32m-> 5180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/datasets/io/parquet.py:131\u001b[0m, in \u001b[0;36mParquetDatasetWriter.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         written \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write(file_obj\u001b[38;5;241m=\u001b[39mbuffer, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparquet_writer_kwargs)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     written \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet_writer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m written\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/datasets/io/parquet.py:155\u001b[0m, in \u001b[0;36mParquetDatasetWriter._write\u001b[0;34m(self, file_obj, batch_size, **parquet_writer_kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset), batch_size),\n\u001b[1;32m    147\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mba\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    148\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating parquet from Arrow format\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    149\u001b[0m ):\n\u001b[1;32m    150\u001b[0m     batch \u001b[38;5;241m=\u001b[39m query_table(\n\u001b[1;32m    151\u001b[0m         table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m_data,\n\u001b[1;32m    152\u001b[0m         key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(offset, offset \u001b[38;5;241m+\u001b[39m batch_size),\n\u001b[1;32m    153\u001b[0m         indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m_indices,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     written \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[1;32m    157\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/pyarrow/parquet/core.py:1116\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[0;34m(self, table, row_group_size)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1112\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1113\u001b[0m            \u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m.\u001b[39mschema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema))\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/pyarrow/_parquet.pyx:2229\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.write_table\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/vl-hf-workflow/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2: In chunk 0: Invalid: First or last list offset out of bounds"
     ]
    }
   ],
   "source": [
    "vl.to_hf(hf, hf_repo_id, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vl-hf-workflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
